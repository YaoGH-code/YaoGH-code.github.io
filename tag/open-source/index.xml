<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Open Source | Yao Xu</title>
    <link>http://yaogh-code.github.io/tag/open-source/</link>
      <atom:link href="http://yaogh-code.github.io/tag/open-source/index.xml" rel="self" type="application/rss+xml" />
    <description>Open Source</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 02 May 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://yaogh-code.github.io/media/icon_hu3fa31baf23334842a6c8f8d4ce717078_16335_512x512_fill_lanczos_center_3.png</url>
      <title>Open Source</title>
      <link>http://yaogh-code.github.io/tag/open-source/</link>
    </image>
    
    <item>
      <title>An Operating System for Multicore RISC-V Systems</title>
      <link>http://yaogh-code.github.io/post/os/</link>
      <pubDate>Tue, 02 May 2023 00:00:00 +0000</pubDate>
      <guid>http://yaogh-code.github.io/post/os/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>A Preemptive User-Level Thread Library</title>
      <link>http://yaogh-code.github.io/post/uthread/</link>
      <pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate>
      <guid>http://yaogh-code.github.io/post/uthread/</guid>
      <description>&lt;h2 id=&#34;repo-link&#34;&gt;Repo Link&lt;/h2&gt;
&lt;!-- https://github.com/YaoGH-code/uthread --&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;This project aims to provide a robust preemptive user-level thread library that simplifies the creation and management of user-level threads. By utilizing interfaces such as &amp;ldquo;uthread_create&amp;rdquo; and &amp;ldquo;uthread_join,&amp;rdquo; users familiar with POSIX threads can easily develop programs with user-level parallelism. The context of user-level threads in this project is implemented through a combination of our custom &amp;ldquo;uthread&amp;rdquo; data structure and the &amp;ldquo;ucontext&amp;rdquo; functionality provided by POSIX. Additionally, a lock-free data structure is employed to maintain each worker thread efficiently. The scheduling of user threads relies heavily on signals in this project. When a signal arrives, it indicates the start of the next scheduling round for each worker thread. The stack prepared by the kernel for the signal handler is crucial for smooth context switching. To ensure thread safety at the user level, interfaces such as &amp;ldquo;umalloc,&amp;rdquo; &amp;ldquo;uprintf,&amp;rdquo; and &amp;ldquo;uthread_mutex_lock&amp;rdquo; are provided. These interfaces facilitate safe memory allocation, printing, and mutual exclusion. This thread library is compatible with machines running MacOS or Linux with x86 architecture CPUs. Extensive testing has been conducted to verify the correctness and scalability of the system.&lt;/p&gt;
&lt;!-- 
## Background
With the increasing prevalence of multi-core processors, high-performance computing demands, scalability requirements, user expectations for responsiveness, and cost-efficiency considerations, concurrent programming has become increasingly essential. To achieve concurrency, multi-threading has emerged as a popular approach, primarily due to its ability to leverage the shared memory model and its lower resource consumption. POSIX threads (pthreads) have gained widespread adoption for multi-threading implementations due to their efficiency, scalability, reliability, and broad support across various operating systems. However, effectively harnessing pthreads for efficient concurrency still presents significant challenges.

One challenge arises when the number of concurrent tasks increases, potentially causing significant scheduling overhead and overwhelming hardware resources. The thread pool technique has long been used to address this issue, where it abstracts each concurrent task into a unit of work and adds it to a shared work queue. From this queue, threads in the pool grab one unit of work and finish executing it, then proceed to the next until the queue is empty. The thread pool can be a neat solution to deal with a huge quantity of tasks while keeping the scheduling overhead bounded. However, it may be insufficient when the running spans of each concurrent task are highly variable. This is especially true if we aim to achieve user-level multitasking, where we can have long-running or persistent tasks. The thread pool may not be a wise solution here, as the tasks that run much longer than the others can cause drastic delay to those far down in the queue, which, in the worst case, may never be executed. That’s because in the thread pool model subsequent tasks are not executed until all preceding tasks have been completed. If we were still to use pthreads naively, we would end up spawning hundreds of threads again, one for each task, producing immense overhead and hurting performance.

Clearly, the thread pool model should be preserved. But how to break the execution of long-running blocking tasks without compromising correctness to allow other tasks to run? To answer this question, we must introduce the idea of user-level threads. In this project, we turn each unit of work in the work queue into an execution context. Each worker thread (POSIX thread) has a task queue stores user level tasks that assigned to it when user create a user level thread with uthread create. These user level tasks are structures where we turn each unit of work in the work queue (we are still sticking to the thread pool model) into an execution context, keeping track of the full state of each concurrently running task, and instead of mapping one task to one dedicated POSIX thread, we dynamically decide which POSIX thread in the pool should execute which task, so that we can safely break the execution of one task, saving its context back to the shared queue, and start the execution of the next user-level thread, loading the corresponding context from the queue.

## Approach
Before going into the core part of this project, some initialization and helper function will be introduced first. This user level thread library requires some initialization before starting to create and schedule user level threads. The runtime start function shown below is the first function will be called to initialize a structure called runtime which is defined in uthread.c file that contains some run time information. First, there will be only one thread enter this function. We record this main thread as the master worker thread (pthread) and mark the start flag to one so that the runtime will only be initialized once. Then, in the for loop below, as many worker threads (pthread) as there are CPU cores will be created. From a perspective of the user level thread library, these worker threads are managed by struct worker. There are three important data structure associated with each worker thread which are ID, pthread id and a linked list contains struct uthread. ID is a number from 0 to number of cores - 1 which is assigned when creating the worker thread. pthread id is just a pthread t type number which returned when pthread create function is called. The linked list contains structures (struct uthread) that describes each user level thread assigned to this worker. How user level thread is defined and implemented in this library will covered later in much more details. (line 1-line 28) After initializing the run time structure and all worker threads, we installed two signal handlers for SIGALRM 3 and SIGUSR1 for worker threads. SIGALRM combined with SIGUSR1 is acting as a timer which is used to notify worker threads to schedule a new user level thread from its linked list. (line 31-line 58)
```C
void runtime_start() {
    runtime.master = pthread_self();
    runtime.started = 1;

    // get core count
    runtime.worker_count = (uint32_t)sysconf(_SC_NPROCESSORS_ONLN);

    // start as many works as cores
    // allocate memory for workers
    runtime.workers = _umalloc(sizeof(struct worker) * runtime.worker_count);
    // initialize workers
    for (int id = 0 ; id &lt; runtime.worker_count; id++) {
        struct worker *w = &amp;runtime.workers[id];
        // install worker id
        w-&gt;id = id;
        // initialize the work queue
        // create a dummy uthread that will stay as the head of the work queue
        w-&gt;head = _umalloc(sizeof(struct uthread));
        // initialize the dummy uthread
        // this dummny uthread does not have an id and will never be exposed to the user
        w-&gt;head-&gt;next = w-&gt;head; // circular queue
        memset(&amp;w-&gt;head-&gt;ucon, 0 , sizeof(ucontext_t));
        // set the dummy uthread as the current thread in execution
        w-&gt;head-&gt;state = USTATE_RUNNING;
        w-&gt;cur = w-&gt;head;
        // start the pthread
        pthread_create(&amp;w-&gt;pthread_id, NULL, dummy, (void *)(uint64_t)id);
    }

    // install signal handlers
    struct sigaction sa_alrm, sa_usr1;

    memset(&amp;sa_alrm, 0, sizeof(sa_alrm));
    memset(&amp;sa_usr1, 0, sizeof(sa_usr1));
    
    sa_usr1.sa_flags = SA_SIGINFO | SA_RESTART;
    sa_usr1.sa_sigaction = scheduler;
    sigfillset(&amp;sa_usr1.sa_mask);
    sigaction(SIGUSR1, &amp;sa_usr1, NULL);

    sa_alrm.sa_flags = SA_RESTART;
    sa_alrm.sa_handler = sigalrm_handler;
    sigfillset(&amp;sa_alrm.sa_mask);
    sigaction(SIGALRM, &amp;sa_alrm, NULL);

    // create a periodic timer
    struct itimerval timer;

#define MS (1000)
    // Configure the timer to fire every 10ms
    timer.it_value.tv_sec = 0;
    timer.it_value.tv_usec = MS * 10;
    timer.it_interval.tv_sec = 0;
    timer.it_interval.tv_usec = MS * 10;

    // start the timer
    while (runtime.ready_count != runtime.worker_count);
    setitimer(ITIMER_REAL, &amp;timer, NULL);
}
```

Following function sigalrm handler defined in uthread.c described how these two signals are used in terms
of scheduling user level threads. SIGALRM is generated by the OS based on the scheduling round period
we defined when setting the timer. Since the OS will send the SIGALRM to a random worker thread that
is running in the current process context and we want all worker threads to know they should schedule the
next user level thread, the thread receives SIGALRM will send a SIGALRM signal to the master worker
thread and this master worker thread will send SIGUSR1 to each of other worker threads to notify them a
new scheduling round comes.

```C
void sigalrm_handler(int signum) {
    if (pthread_self() != runtime.master) { // not master
        pthread_kill(runtime.master, SIGALRM); // forward SIGALRM to the master thread
    } else {
        // bcast SIGUSR1 to all workers to start the scheduler
        for (int i = 0; i &lt; runtime.worker_count; i++) {
            pthread_kill(runtime.workers[i].pthread_id, SIGUSR1);
        }
    }
}
```

The second signal handler which is used to process SIGUSR1 is registered as a function called scheduler. Back to runtime start function, this signal handler is installed by sigaction with a flag called
SA SIGINFO. When the SA SIGINFO flag is specified in act.sa flags, the signal handler address is passed
via the act.sa sigaction field. This handler takes three arguments, as follows:
* &lt;span style=&#34;color:orange&#34;&gt;sig&lt;/span&gt;: The number of the signal that caused invocation of the handler.
* &lt;span style=&#34;color:orange&#34;&gt;info&lt;/span&gt;: A pointer to a siginfo t, which is a structure containing further information about the signal, as described below.
* &lt;span style=&#34;color:orange&#34;&gt;ucontext&lt;/span&gt;: This is a pointer to a ucontext t structure, cast to void *. The structure pointed to by this field contains signal context information that was saved on the user-space stack by the kernel.

This is why our SIGUSR1 handler (scheduler) has three arguments and the third argument is very important because of the following reason:
![Image alt](sighand.png)

The above graph shows the core process of user level scheduling. First, ”UC” stands for ”user level thread context”.

```C
struct uthread {
    int id;
    int state;
    char *stack;
    void *retptr;
    void *aux_rsp;
    char *aux_stack;
    ucontext_t ucon;
    struct uthread *next;
};
```
User level thread context is maintained with a structure called struct uthread defined in uthread.c. Each user level thread has its own ID, state, stack, a ucontext t struct and a pointer to the next struct uthread in the current queue. ucontext t is a structure defined by POSIX used to save context including general purpose register and SIMD registers etc. Back to the above graph, when a worker thread received SIGUSR1, the operating system will push three inputs for the signal handler onto its stack which can be used within the signal handler. The third one is the new user level context that was running on CPU. It will be casted to a ucontext t type variable and saved back to the user level thread context queue bu the scheduler. It will also push the next scheduled user level thread context to the stack to replace the user level thread context pushed by the OS. When signal handler returns, a system call called sigreturn will store the new context
back to CPU and continue execuation, and user level context switch is achieved.

Two interfaces most related to users are uthread_create and uthread_join. The following chunk of code is the implementation of the uthread_create function. For each newly created user level thread, a stack is allocated on the heap for later execution. The execution context will also be prepared by setting RSP to newly allocated stack, setting RDI to input and setting RIP to the function that user would run. On Linux, code segment register and stack segment register are also need to be set manually since Linux will restore context to CPU with value specified in the ucontext_t structure including CS and SS register. On Linux, there is a mcontext_t structure defined inside of the ucontext_t structure under usr/include/x86_64-linux-gnu/sys/ucontext.h. This structure defines all registers that can be managed by using ucontext. User can access or change REG CSGSFS to manage CS and SS register. However, there is a invisible padding inside of this field to user which took us a while to figure out how to store CS and SS register. Unlike Linux, MacOS manage CS and SS register automatically.

Finally, a round robin algorithm is used to assign new user context to work thread. A lock free operation is used to add newly created user level context after the first element in a worker’s queue by using _cas function defined in asm.s.

```C

#define UTHREAD_STACK_SIZE (1024 * 1024 * 8)
#define UTHREAD_SCRATCH_SPACE_SIZE (1024 * 4)
void uthread_create(uthread_t *id, void *(*func)(void *), void *arg) {

    if (!runtime.started)
        runtime_start();

    struct uthread *u = _umalloc(sizeof(struct uthread));
    memset(&amp;u-&gt;ucon, 0, sizeof(ucontext_t));
    // assign id
    u-&gt;id = runtime.next_uid++;

    // allcoate stack on heap
    u-&gt;stack = _umalloc(UTHREAD_STACK_SIZE);
    // initailze the stack
    uint64_t *rsp = (uint64_t *)(ALIGN16(u-&gt;stack + UTHREAD_STACK_SIZE) - 8);
    // prepare the return address
    *rsp = (uint64_t)cleanup;

    // allocate the aux stack
    u-&gt;aux_stack = _umalloc(UTHREAD_SCRATCH_SPACE_SIZE);
    // initailize the aux stack
    u-&gt;aux_rsp = (void *)(ALIGN16(u-&gt;aux_stack + UTHREAD_SCRATCH_SPACE_SIZE)); // do not subtract 8 here

    // initialize the execution context
    u-&gt;ucon.uc_mcontext.gregs[REG_RSP] = (greg_t)rsp;
    u-&gt;ucon.uc_mcontext.gregs[REG_RDI] = (greg_t)arg;
    u-&gt;ucon.uc_mcontext.gregs[REG_RIP] = (greg_t)func;
    u-&gt;ucon.uc_mcontext.gregs[REG_CSGSFS] = _cs() | ((uint64_t)_ss() &lt;&lt; 48);

    // initialize the state
    u-&gt;state = USTATE_SLEEPING;

    // pick a worker and insert the uthread into ist work queue
    struct worker *w = &amp;runtime.workers[runtime.next_worker++ % runtime.worker_count];
    struct uthread *head = w-&gt;head;
    // add to the work queue (always add after the head)
    struct uthread *old_next;
    do {
        old_next = head-&gt;next; // the next uthread of the head is guaranteed to be valid
        u-&gt;next = old_next;
    } while (_cas(&amp;head-&gt;next, (uint64_t)old_next, (uint64_t)u) != (uint64_t)old_next);

    *id = (uthread_t)u;
}
```
There are three function in this library are about resource recycling. cleanup function is the return address after of each user level thread, it set the current user level thread to USTATE_JOINABLE. User level thread with USTATE JOINABLE won’t be recycled immediately. After uthread join is called, user level thread will be set to USTATE_JOINED.

In each worker’s queue, the dummy head is a user level context that is responsible for freeing resources. When
this user level context is scheduled, it will look for user level context that is in USTATE_JOINABLE and
USTATE_JOINED. If the uthread is in USTATE_JOINABLE, the stack will freed because the JOINABLE
uthread cannot possibly be running since they are mapped to a single worker who’s only able to do one
thing at a time. That means the uthread will no longer be schedulable from now on because it can only be
run if the scheduler was to schedule it later, which will not happen, whereby it is safe to free its stack and
execution context. If the uthread is in USTATE_JOINED, the struct uthread will be freed since when dummy
is running, the scheduler must not be running since they again share the same worker, thus, if dummy sees
one uthread in the JOINED state, that uthread cannot be scheduled to run again, since first the scheduler is
uninterruptible, which eliminates the intermediate state being observed by dummy and second the next time
scheduler tries to schedule it will definitely also see it as JOINED and do nothing. However, one thing to
notice is that we have to make the JOINED uthread unreachable and then deallocate the memory otherwise,
when the scheduler step through it may encounter invalid memory.

![Image alt](queue.png)

The following graph shows the life cycle of a user level thread. In summary, a stack and other necessary data structures are allocated on the heap first. Then, the ucontext structure will be added into the queue of one of the workers and waiting to be scheduled. After execuation finished, it will jump to clean up function
and wait to be joined and resource recycled.

![Image alt](lifecycle.png)

Here I wish to mention a few obstacles we have met till now. Initially, we thought of implementing work queues using doubly-linked lock-free lists; however, it turned out that there had been yet no known solutions to this problem, which would not have been if a compare-and-swap instruction had been given by the architecture that can operate on two discounted addresses, and most of the articles and papers being widely accepted were just singly-linked lists. Thus, we decided to not go any further but to fall back to singly-linked lists, which did not appear to be any simpler, especially when it came to the deleting part, investigating which we got to know about the differences between lock-free and wait-free. After that, we decided to get around that, avoiding a deletion from happening in parallel with insertions or another deletions. That was achieved by constraining the insertion to only taking place between the head of the list and the second to the head. As for the deletion, we spawn another user thread during the run-time initialization that&#39;s not exposed to the user and is solely responsible for performing the deletion, and we call it the garbage collector, and an important constraint we put on this to prevent deletions and insertions from interfering each other on the second node, we forced the deletion to always start from the third node. In this way, we have a lock-free singly-linked list that can be added to in a concurrent lock-free manner. Then, when implementing uthread\_join, and uthread\_detach that involed traversing the work queue to verify the validity of a given uthread ID, we considered using the Linux RCU to better read-mostly efficiency and implemented it, but we eventually decided to revert back to having the user responsible for properly managing the IDs since the help of the RCU in our case was not conspicuously seen while it largely added to the code complexity and made the code potentially more error-prone in later stages. Then we faced the serious issue of not freeing the memory fast enough to make room for newly spawned threads, to which we came up with the solution that has each uthread free its own stack during the clean-up phase (a snippet of inline Assembly freeing the stack and saving the return value without needing a stack via using munmap syscall exploiting the fact that the syscall happens on the kernel stack).

## Results

First test (following two graphs) is using fixed amount of uthread with different number worker thread to check the scalability of this library on machines with different of number of cores. We created 4000 uthreads that each calculates Fib 30 with different number of workers. This test is done on GHC machines(CPU: i7-9700 8 cores). 
Figure 4 shows the total execution time of Fib 30 with 4000 uthreads and 1,2,3,4,5,6 and 7 workers. Figure 5 shows the linear speed up in term of doing same amount of work when we are using more workers, which meet our expectation since more workers means more execution unit available for uthreads and more concurrency, just like more cores available for pthreads. It shows the correctness of user level context switching, efficiency of the scheduling mechanism and good scalability of this library on a 8 core machine. 

![Image alt](Execution_time.png)

![Image alt](speedup.png)
The second test is using fixed amount of workers with different number of uthreads to check the scalability of this library. We created 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000 uthreads that each calculates Fib 30 based on same number of workers. This test is done on GHC machines. Figure 6 shows the execution time of Fib 30 with 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000 uthreads and 7 workers. We can see the execution time is increasing in a linear way. This means when users are creating more uthreads, the system can provide stable scheduling and performance without meeting any memory or scheduling bottleneck. In extreme case, a system with same hardware configuration as GHC machines can support 200000 uthreads running at the same time.

![Image alt](exe_time.png)


## Reference
Sundell, H., &amp; Tsigas, P. (2008). Lock-free deques and doubly linked lists. Journal of Parallel and Distributed
Computing, 68(7), 1008–1020. https://doi.org/10.1016/j.jpdc.2008.03.001







 --&gt;
</description>
    </item>
    
    <item>
      <title>A Simulator of Virtual Memory and Process Scheduling</title>
      <link>http://yaogh-code.github.io/post/vmp_sim/</link>
      <pubDate>Sun, 30 Apr 2023 02:00:00 +0000</pubDate>
      <guid>http://yaogh-code.github.io/post/vmp_sim/</guid>
      <description>&lt;h2 id=&#34;repo-link&#34;&gt;Repo Link:&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/YaoGH-code/MemSim&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/YaoGH-code/MemSim&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>A 5-stage pipeline MIPS CPU</title>
      <link>http://yaogh-code.github.io/post/mcpu/</link>
      <pubDate>Sun, 30 Apr 2023 00:00:00 +0000</pubDate>
      <guid>http://yaogh-code.github.io/post/mcpu/</guid>
      <description>&lt;h2 id=&#34;repo-link&#34;&gt;Repo Link&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/YaoGH-code/mCPU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/YaoGH-code/mCPU&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The MIPS (Microprocessor without Interlocked Pipeline Stages) architecture is a reduced instruction set computer (RISC) architecture that has played a significant role in the development of microprocessors.
MIPS architecture features a clean and streamlined instruction set, emphasizing simplicity and efficiency. It focuses on optimizing the performance of frequently used instructions, making it suitable for a wide range of applications, including embedded systems, consumer electronics, networking equipment, and high-performance computing.&lt;/p&gt;
&lt;p&gt;One of the distinguishing characteristics of MIPS is its fixed instruction format, where instructions are encoded in a 32-bit word. This fixed-format allows for efficient decoding and pipelining, enabling high-performance execution and reducing the complexity of the microarchitecture.
MIPS processors employ a five-stage pipeline, including instruction fetch, instruction decode, execution, memory access, and write-back. This pipelining technique enables instructions to be processed concurrently, maximizing the overall throughput of the processor.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Image alt&#34; srcset=&#34;
               /post/mcpu/pipeline_hudb3132c6a65292d7bcc467d1157af924_49776_93e1b4c91cf3ca7a05af67e3502d6d62.webp 400w,
               /post/mcpu/pipeline_hudb3132c6a65292d7bcc467d1157af924_49776_6f0b1e1333cee2e1fc091fa2c2f7e0c9.webp 760w,
               /post/mcpu/pipeline_hudb3132c6a65292d7bcc467d1157af924_49776_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;http://yaogh-code.github.io/post/mcpu/pipeline_hudb3132c6a65292d7bcc467d1157af924_49776_93e1b4c91cf3ca7a05af67e3502d6d62.webp&#34;
               width=&#34;637&#34;
               height=&#34;233&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;design&#34;&gt;Design&lt;/h2&gt;
&lt;p&gt;The picture above shows the structure and components of this CPU. This CPU is composed of 5 stages, which are: Instruction Fetch(IF), Instruction Decode(ID), Execution(EXE), Memory(MEM) and Write Back(WB) stage.&lt;/p&gt;
&lt;h3 id=&#34;if-stage&#34;&gt;IF Stage&lt;/h3&gt;
&lt;p&gt;The Instruction Fetch stage consists of three components. Firstly, there is the program counter register, which holds the physical address of the next instruction in the instruction memory (represented here as a simplified memory structure). In each cycle, an adder increments the address by 4, and the resulting address is stored in the program counter for the next clock cycle. Simultaneously, the instruction memory, equipped with an input port and an output port, fetches an instruction using the address from the program counter. This fetched instruction is then stored in the IF/ID register during the same cycle.&lt;/p&gt;
&lt;h3 id=&#34;id-stage&#34;&gt;ID Stage&lt;/h3&gt;
&lt;p&gt;In the instruction decoding stage, we can clearly see the connection between the instruction set and the actual electronic circuit. First, the instruction will be decoded into following components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;op code: basic operation of the instruction&lt;/li&gt;
&lt;li&gt;rs: the first register source operand&lt;/li&gt;
&lt;li&gt;rt: the second register source operand&lt;/li&gt;
&lt;li&gt;rd: the register destination operand&lt;/li&gt;
&lt;li&gt;shamt: shift amount to be used in shift instructions&lt;/li&gt;
&lt;li&gt;funct code: function code&lt;/li&gt;
&lt;li&gt;imm: immediate value&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Image alt&#34; srcset=&#34;
               /post/mcpu/inst_hu15dfa85bd2086bc24f5770631c7f3697_1512767_6929362b380fbac60f299e210dda9ef7.webp 400w,
               /post/mcpu/inst_hu15dfa85bd2086bc24f5770631c7f3697_1512767_1f333696f8a24b945d22e98f5d2b79da.webp 760w,
               /post/mcpu/inst_hu15dfa85bd2086bc24f5770631c7f3697_1512767_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;http://yaogh-code.github.io/post/mcpu/inst_hu15dfa85bd2086bc24f5770631c7f3697_1512767_6929362b380fbac60f299e210dda9ef7.webp&#34;
               width=&#34;760&#34;
               height=&#34;466&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Combining the Op code (bits 31-26) and funct code (bits 5-0) in MIPS instructions enables the hardware to determine the current instruction and its corresponding function. For example, instructions for basic arithmetic operation(R-format) have the same op code 0x0 but
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Image alt&#34; srcset=&#34;
               /post/mcpu/format_hu05f80807f171e7fa4052f186c7c56bbd_336070_9a02269c4b175473d707fbddc06246b3.webp 400w,
               /post/mcpu/format_hu05f80807f171e7fa4052f186c7c56bbd_336070_a9cdcd147be056fb04ed6476171de7c4.webp 760w,
               /post/mcpu/format_hu05f80807f171e7fa4052f186c7c56bbd_336070_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;http://yaogh-code.github.io/post/mcpu/format_hu05f80807f171e7fa4052f186c7c56bbd_336070_9a02269c4b175473d707fbddc06246b3.webp&#34;
               width=&#34;760&#34;
               height=&#34;117&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

with a different funct code. So, Op code and funct code combined will be provided to the control unit for generating control signals for later stages. These control signals will be covered later.&lt;/p&gt;
&lt;p&gt;There is also a MUX in the MIPS architecture that utilizes the control signal &amp;ldquo;regrt&amp;rdquo; to select either the &amp;ldquo;rd&amp;rdquo; or &amp;ldquo;rt&amp;rdquo; register. The selected signal, along with the data to be written back to the register file, is then passed back to the ID stage. This MUX serves the purpose of determining the appropriate destination for the write-back operation. For instance, instructions with an opcode of 0 will select the &amp;ldquo;rd&amp;rdquo; register as the destination. On the other hand, for the LW (Load Word) instruction, the data retrieved from the data memory during the MEM stage will be written back to the register specified by the &amp;ldquo;rt&amp;rdquo; register. Therefore, the MUX helps decide which part of the instruction contains the desired destination for the write-back operation.&lt;/p&gt;
&lt;p&gt;The signals &amp;ldquo;rs&amp;rdquo; and &amp;ldquo;rt&amp;rdquo; are used to access the register file and read the data stored in registers &amp;ldquo;rs&amp;rdquo; and &amp;ldquo;rt&amp;rdquo; simultaneously through the &amp;ldquo;qa&amp;rdquo; and &amp;ldquo;qb&amp;rdquo; ports. Additionally, the immediate value (imm) is sign extended and passed to the ALU for computation. This computation takes place during the EXE stage, where the ALU performs operations using the immediate value and other relevant data.&lt;/p&gt;
&lt;p&gt;Then, let&amp;rsquo;s look at the control signals generated by the control unit:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;wreg: A write back to register will happen in the lifecycle of the current instruction&lt;/li&gt;
&lt;li&gt;m2reg: A write back to register with data in memory will happen in the lifecycle of the current instruction&lt;/li&gt;
&lt;li&gt;wmem: A write to memory will happen in the lifecycle of the current instruction&lt;/li&gt;
&lt;li&gt;aluc: ALU operation mode ex. if aluc==b0010, then ALU_out &amp;lt;= ALU_a + ALU_b;&lt;/li&gt;
&lt;li&gt;aluimm: A immediate value is needed by the ALU to finish this operation&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;exe-stage&#34;&gt;EXE Stage&lt;/h3&gt;
&lt;p&gt;In the EXE (Execute) stage, two signals are employed to control the ALU&amp;rsquo;s input sources and operation mode. Firstly, the &amp;ldquo;ealuc&amp;rdquo; signal is utilized to select the desired operation mode for the ALU. For instance, a value of &amp;ldquo;b0010&amp;rdquo; would indicate that the ALU should perform an addition operation on its two input numbers.&lt;/p&gt;
&lt;p&gt;Secondly, the &amp;ldquo;ealuimm&amp;rdquo; signal determines the source of the input port &amp;ldquo;b&amp;rdquo; of the ALU. This is necessary because the number on port &amp;ldquo;b&amp;rdquo; can either come from the register file or be an extended immediate value. The &amp;ldquo;ealuimm&amp;rdquo; signal helps in determining whether the value on port &amp;ldquo;b&amp;rdquo; should be fetched from the register file or be the extended immediate value.&lt;/p&gt;
&lt;h3 id=&#34;mem-stage&#34;&gt;MEM Stage&lt;/h3&gt;
&lt;p&gt;During the MEM (Memory) stage, the &amp;ldquo;mwmem&amp;rdquo; signal is employed to determine whether the current instruction requires writing to the data memory or not. If the &amp;ldquo;mwmem&amp;rdquo; signal is set to indicate a write operation, the value from input port &amp;ldquo;a&amp;rdquo; will be written to the memory address specified by the value on input port &amp;ldquo;di&amp;rdquo;. On the other hand, if the &amp;ldquo;mwmem&amp;rdquo; signal indicates a read operation, the word at the memory address specified by the value on input port &amp;ldquo;di&amp;rdquo; will be retrieved and passed to the output port of the data memory. The &amp;ldquo;mwmem&amp;rdquo; signal thus controls the write or read operation on the data memory based on the instruction&amp;rsquo;s requirements.&lt;/p&gt;
&lt;h3 id=&#34;wb-stage&#34;&gt;WB Stage&lt;/h3&gt;
&lt;p&gt;In the final stage of the MIPS pipeline, the &amp;ldquo;wm2reg&amp;rdquo; (also referred to as &amp;ldquo;em2reg&amp;rdquo;) signal is employed as a selector for a MUX. This MUX determines whether the result obtained from the ALU computation or the data recently read from the data memory during the MEM stage will be chosen for writing back to the register file. The &amp;ldquo;wm2reg&amp;rdquo; signal serves as the control signal for this MUX, enabling the selection of the appropriate data to be written back based on the pipeline stage&amp;rsquo;s requirements.&lt;/p&gt;
&lt;h2 id=&#34;testing&#34;&gt;Testing&lt;/h2&gt;
&lt;p&gt;RTL design:
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Image alt&#34; srcset=&#34;
               /post/mcpu/RTL_hu065ead26b1d81c746475df4a32486d4a_25121_7d428c2d05928696796cefbc52f90ac6.webp 400w,
               /post/mcpu/RTL_hu065ead26b1d81c746475df4a32486d4a_25121_559f22455183e76c554b3ac0f4530197.webp 760w,
               /post/mcpu/RTL_hu065ead26b1d81c746475df4a32486d4a_25121_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;http://yaogh-code.github.io/post/mcpu/RTL_hu065ead26b1d81c746475df4a32486d4a_25121_7d428c2d05928696796cefbc52f90ac6.webp&#34;
               width=&#34;695&#34;
               height=&#34;365&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Output waveform of executing following instructions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;32&amp;rsquo;b10001100001000100000000000000000; -&amp;gt; lw $2, 00($1)&lt;/li&gt;
&lt;li&gt;32&amp;rsquo;b10001100001000110000000000000100; -&amp;gt; lw $3, 04($1)&lt;/li&gt;
&lt;li&gt;32&amp;rsquo;b10001100001001000000000000001000; -&amp;gt; lw $4, 08($1)&lt;/li&gt;
&lt;li&gt;32&amp;rsquo;b10001100001001010000000000001100; -&amp;gt; lw $5, 12($1)&lt;/li&gt;
&lt;li&gt;32&amp;rsquo;b00000000010010100011000000100000; -&amp;gt; add $6, $2, $10
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Image alt&#34; srcset=&#34;
               /post/mcpu/WF_hu65981b8fd6aec216c24366d4c8ea2367_150116_89fdd5cfc51f9cb44f46551d323c22c3.webp 400w,
               /post/mcpu/WF_hu65981b8fd6aec216c24366d4c8ea2367_150116_f054340b8581abc60eb343afe825eed1.webp 760w,
               /post/mcpu/WF_hu65981b8fd6aec216c24366d4c8ea2367_150116_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;http://yaogh-code.github.io/post/mcpu/WF_hu65981b8fd6aec216c24366d4c8ea2367_150116_89fdd5cfc51f9cb44f46551d323c22c3.webp&#34;
               width=&#34;760&#34;
               height=&#34;418&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
